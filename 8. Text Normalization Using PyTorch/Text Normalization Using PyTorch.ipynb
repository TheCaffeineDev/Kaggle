{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from io import open\nimport unicodedata\nimport string\nimport re\nimport random\n\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nimport numpy as np\n%matplotlib inline\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca57f6702e6cb13ce660d4a36bd7d37cd177a046"
      },
      "cell_type": "code",
      "source": "\ndf = pd.read_csv('../input/en_train.csv',encoding='utf-8')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e50231d2a594f6949eb02152b70003e64f4bde0d"
      },
      "cell_type": "code",
      "source": "df = df[df['class']=='DATE']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aaf3185381231475d9749c9531b4d5e3b8ce419a"
      },
      "cell_type": "code",
      "source": "df.head(30)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8b5c78d4ca370aaa241039b7f4be8b615a12e14"
      },
      "cell_type": "code",
      "source": "\nMAX_LENGTH = df.after.str.len().max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "646eb32b597ad46453755d6ddf6b19cf2a76dfe6"
      },
      "cell_type": "code",
      "source": "SOS_token = 0\nEOS_token = 1\n\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.char2index = {}\n        self.char2count = {}\n        self.index2char = {0: \"SOS\", 1: \"EOS\"}\n        self.n_chars = 2  # Count SOS and EOS\n\n    def add_sentence(self, sentence):\n        for char in sentence:\n            self.add_char(char)\n\n    def add_char(self, char):\n        if char not in self.char2index:\n            self.char2index[char] = self.n_chars\n            self.char2count[char] = 1\n            self.index2char[self.n_chars] = char\n            self.n_chars += 1\n        else:\n            self.char2count[char] += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f497ac4d125866ad181f4a6786d1ecc3b59c908b"
      },
      "cell_type": "code",
      "source": "def readLangs(df,lang1, lang2, reverse=False):\n    pairs = [[a,b] for b,a in zip(df[lang1],df[lang2])]\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8926265f7f849f21b4a4aed8cb5b15b92a43c20c"
      },
      "cell_type": "code",
      "source": "def prepareData(df,lang1, lang2, reverse=False):\n    MAX_LENGTH = df[lang2].str.len().max()\n    input_lang, output_lang, pairs = readLangs(df,lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    print(\"Counting chars...\")\n    for pair in pairs:\n        input_lang.add_sentence(pair[0])\n        output_lang.add_sentence(pair[1])\n    print(\"Counted chars:\")\n    print(input_lang.name, input_lang.n_chars)\n    print(output_lang.name, output_lang.n_chars)\n    return input_lang, output_lang, pairs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc0b220579c7ebace6399e6aaa011a51d19bc9d6"
      },
      "cell_type": "code",
      "source": "\ninput_lang, output_lang, pairs = prepareData(df,'before', 'after', True)\nprint(random.choice(pairs))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9d9d7c7ebb95a0effe440c34a07b17fbbfe5892"
      },
      "cell_type": "code",
      "source": "class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size) # embedding_size (vocab_size), embedding_dim\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f7bdbc91d54dba85dab40d0058fcd93f9aadb57"
      },
      "cell_type": "code",
      "source": "class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n                                 encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5fb18d8e264eb840551ab1bcb9ffcbeaa27048d"
      },
      "cell_type": "code",
      "source": "def indexesFromSentence(lang, sentence):\n    return [lang.char2index[char] for char in sentence]\n\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca3d3256a70156e8c7d7c17f6f540ed7adc2e7eb"
      },
      "cell_type": "code",
      "source": "teacher_forcing_ratio = 0.5\n\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfdcba5214c263318ff97ca53fca93dbf1612a64"
      },
      "cell_type": "code",
      "source": "######################################################################\n# This is a helper function to print time elapsed and estimated time\n# remaining given the current time and progress %.\n#\n\nimport time\nimport math\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b8a7022f631811845c980b2810482482c75f793"
      },
      "cell_type": "code",
      "source": "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    training_pairs = [tensorsFromPair(random.choice(pairs))\n                      for i in range(n_iters)]\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        training_pair = training_pairs[iter - 1]\n        input_tensor = training_pair[0]\n        target_tensor = training_pair[1]\n\n        loss = train(input_tensor, target_tensor, encoder,\n                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total / print_every\n            print_loss_total = 0\n            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n                                         iter, iter / n_iters * 100, print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total / plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66bc7091d930adc6582b82314952afa53a96e59e"
      },
      "cell_type": "code",
      "source": "def showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "204e71f23ce9bbfa0c198717c0d59aed81ac50d0"
      },
      "cell_type": "code",
      "source": "\ndef evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n                                                     encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_chars = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_chars.append('<EOS>')\n                break\n            else:\n                decoded_chars.append(output_lang.index2char[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_chars, decoder_attentions[:di + 1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3653187653f8ffdc62cb0ca62eec0409a178178"
      },
      "cell_type": "code",
      "source": "def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ''.join(output_words)\n        print('<', output_sentence)\n        print('')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2dd58127f04826d3d06c23d70752afe48cf5d7dd"
      },
      "cell_type": "code",
      "source": "hidden_size = 256\nencoder1 = EncoderRNN(input_lang.n_chars, hidden_size).to(device) # Size of the vocabulary, i.e. maximum integer index + 1.\nattn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_chars, dropout_p=0.1).to(device)\n\ntrainIters(encoder1, attn_decoder1, 50000, print_every=5000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d09c753437c89b1a918b1ee2252676fec5972711"
      },
      "cell_type": "code",
      "source": "trainIters(encoder1, attn_decoder1, 50000, print_every=5000,learning_rate=0.005)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "197adf4f708297e407c95b3a1e1220bd31f98906"
      },
      "cell_type": "code",
      "source": "\nevaluateRandomly(encoder1, attn_decoder1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e724a774f2dc1cfdc6ae59c434ac592962e47b2f"
      },
      "cell_type": "code",
      "source": "output_words, attentions = evaluate(\n    encoder1, attn_decoder1, \"2013\")\nplt.matshow(attentions.numpy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c43b70ecdfedd5bcd0a385aee5de34fdb101b66e"
      },
      "cell_type": "code",
      "source": "def plot_head_map(mma, target_labels, source_labels):\n    fig, ax = plt.subplots()\n    heatmap = ax.pcolor(mma, cmap=plt.cm.Blues)\n\n    # put the major ticks at the middle of each cell\n    ax.set_xticks(np.arange(mma.shape[1])+0.5, minor=False)\n    ax.set_yticks(np.arange(mma.shape[0])+0.5, minor=False)\n\n    # without this I get some extra columns rows\n    # http://stackoverflow.com/questions/31601351/why-does-this-matplotlib-heatmap-have-an-extra-blank-column\n    ax.set_xlim(0, len(source_labels)+2)\n    ax.set_ylim(0, len(target_labels)+1)\n\n    # want a more natural, table-like display\n    ax.invert_yaxis()\n    ax.xaxis.tick_top()\n\n    # source words -> column labels\n    ax.set_xticklabels(['']+list(source_labels), minor=False)\n    # target words -> row labels\n    ax.set_yticklabels(list(target_labels), minor=False)\n\n    plt.yticks(rotation=90)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1adfdc4388a8b0074ca1609c7ca83cea17db2842"
      },
      "cell_type": "code",
      "source": "\ndef evaluateAndShowAttention(input_sentence):\n    output_words, attentions = evaluate(\n        encoder1, attn_decoder1, input_sentence)\n    print('input =', input_sentence)\n    print('output =', ''.join(output_words))\n    print(attentions.shape)\n    plot_head_map(attentions,output_words,input_sentence)\n\n\nevaluateAndShowAttention(\"4 March 2014\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "572e931754fc2b05b8b5cdd1bed78cb4a4785e73"
      },
      "cell_type": "code",
      "source": "evaluateAndShowAttention(\"1982\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "820104651e4cd25e0aa7f19de6d4368ef5cf96f8"
      },
      "cell_type": "code",
      "source": "evaluateAndShowAttention(\"August 16, 2005\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f7062538de0eeaf5cf3c63cc8ee23f1ba7fac91"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}